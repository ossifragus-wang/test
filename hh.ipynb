{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to maximum likelihood estimation\n",
    "\n",
    "## A simple example: Coin tosses\n",
    "\n",
    "-   Coin tosses have binary outcomes: a Head (H) or a Tail (T).\n",
    "-   Assume that different coin tosses don‚Äôt impact each other.\n",
    "-   In statistics, this implies that coin toss outcomes are independent\n",
    "    and identically distributed, or i.i.d..\n",
    "-   Assume that the coin is biased.\n",
    "-   Let the probability of getting a Head be $p$ and the probability of\n",
    "    getting a Tail be `1-p`.\n",
    "-   So how do we find that value of $p$?\n",
    "\n",
    "Let‚Äôs toss the coin five times, and assume that we get the following\n",
    "sequence: H,T,T,H,H.\n",
    "\n",
    "The probability of seeing this result is\n",
    "$$L(p) = p(1-p)(1-p)pp = p^3(1-p)^2$$ where `3` is the number of Heads\n",
    "and 2 is the number of Tails. More generally, if we have a total of $N$\n",
    "tosses, out of which $n$ are Heads, then the probability is written in a\n",
    "generic function form: $$L(p) = p^n(1-p)^{N-n}$$\n",
    "\n",
    "-   Here, $L(p)$ is the likelihood of ‚Äúobserving‚Äù the data.\n",
    "-   It is a function of the unknown parameter $p$.\n",
    "-   Intuitively, it makes sense to use the value, $\\hat{p}$, that\n",
    "    maximizes $L(p)$ to estimate $p$.\n",
    "-   The maximum likelihood estimator (MLE) is\n",
    "    $$\\hat{p}=\\arg\\max_{p}L(p).$$\n",
    "\n",
    "The estimation problem now becomes an optimization problem.\n",
    "\n",
    "-   Here we can differentiate $L$ with respect to $p$ and set it equal\n",
    "    to zero to find the optimal value of $p$ $$L'(p)= \\frac{dL}{dp}=0$$\n",
    "-   This will give us this ugly expression:\n",
    "    $$L'(p)=np^{n-1}(1-p)^{N-n} - (N-n)(1-p)^{N-n-1}p^n=0$$\n",
    "-   This equation is not easy to solve, both analytically and\n",
    "    numerically.\n",
    "\n",
    "Now we consider the log-likelihood function.\n",
    "\n",
    "-   Let $l(p)=\\log\\{L(p)\\}$, namely,\n",
    "    $$l(p) = n\\log p + (N-n)\\log(1-p).$$\n",
    "-   The maximizer of $L(P)$ is the same as the maximizer of $l(p)$.\n",
    "-   Thus $\\hat{p}$ is the solution to\n",
    "    $$l'(p)= \\frac{dl}{dp}=\\frac{n}{p}-\\frac{N-n}{1-p}=0,$$ which is\n",
    "    $$\\hat{p}=\\frac{n}{N}.$$\n",
    "\n",
    "``` latex\n",
    "\\begin{euqation}\n",
    "  l(p) = n\\log p + (N-n)log(1-p)\n",
    "L'(p)=np^{n-1}(1-p)^{N-n} - (N-n)(1-p)^{N-n-1}p^n=0.\n",
    "\\end{euqation}\n",
    "```\n",
    "\n",
    "Let‚Äôs generate a variable that represents the coin tosses:\n",
    "\n",
    "``` julia\n",
    "n = 3\n",
    "N = 5\n",
    "pÃÇÃÇpÃÇ = n/N\n",
    "```\n",
    "\n",
    "sample statistic population parameter description pÃÇ p-h\n",
    "\n",
    "Since we know that the log likelihood (LL) function equals:\n",
    "`LL(p) = 3 *log(p) + 2 * log(1-p)`, we can also plot this as follows:\n",
    "\n",
    "``` julia\n",
    "üçé=1\n",
    "i = 1\n",
    "while true\n",
    "    global i += 1\n",
    "    if i > 10\n",
    "        break\n",
    "    end\n",
    "end\n",
    "rand(3)\n",
    "```\n",
    "\n",
    "which gives us this figure:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú467‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "For the figure we can see that the LL has a maximum point around 0.6. As\n",
    "we discussed in\n",
    "<a href=\"https://medium.com/the-stata-guide/mata-statas-end-game-5983c0ee11bd\" class=\"cc le\">the Mata guide</a>,\n",
    "one of features we can utilize are `optimize` functions. Let‚Äôs use them\n",
    "to find the maximum point:\n",
    "\n",
    "``` if\n",
    "mata\n",
    "void myfunc(todo, x, y, g, H) {\n",
    " y = 3 * log(x) + 2 * log(1-x)\n",
    " }maxval = optimize_init()\n",
    "optimize_init_which(maxval, \"max\")\n",
    "optimize_init_evaluator(maxval, &myfunc())\n",
    "optimize_init_params(maxval, 0.2)\n",
    "xmax = optimize(maxval)xmax\n",
    "end\n",
    "```\n",
    "\n",
    "Without going too much in the details, here we define a function that we\n",
    "want to maximize, and start the search with an arbitrarily chosen value\n",
    "of 0.2 which we can see is closer to the maximum point in the figure.\n",
    "For such a simple optimization problem, the starting value shouldn‚Äôt\n",
    "really matter. From the code above, we get the value of `xmax = 0.6`,\n",
    "which is what we calculated by hand as well.\n",
    "\n",
    "We can also plot this on the graph above:\n",
    "\n",
    "``` if\n",
    "mata\n",
    "void myfunc(todo, x, y, g, H) {\n",
    " y = 3 * log(x) + 2 * log(1-x)\n",
    " }maxval = optimize_init()\n",
    "optimize_init_which(maxval, \"max\")\n",
    "optimize_init_evaluator(maxval, &myfunc())\n",
    "optimize_init_params(maxval, 0)\n",
    "xmax = optimize(maxval)xmax\n",
    "ymax = 3 * log(xmax) + 2 * log(1-xmax)st_local(\"maxx\", strofreal(xmax))\n",
    "st_local(\"maxy\", strofreal(ymax))\n",
    "endtwoway (function 3 * log(x) + 2 * log(1-x), range(0 1)), ///\n",
    " yline(`maxy') xline(`maxx') ytitle(\"LL\") xlabel(0(0.2)1)\n",
    "```\n",
    "\n",
    "from which we get a LL max value of -3.365:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú467‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "But since Stata has built-in MLE functions (see `help ml`) with fairly\n",
    "extensive documentation, we don‚Äôt need to jump through the hoops to\n",
    "define an optimization problem in Mata. We can simply use the `ml`\n",
    "function. If you see the documentation of this command, it won‚Äôt be\n",
    "surprising if you are immediately confused. This use of this command is\n",
    "a bit convoluted and takes some time to get used to the various options.\n",
    "In order to use `ml` commands, we need to write small programs where we\n",
    "define the LL functions that need to be optimized. In the coin toss\n",
    "case, since we already know the LL function which is linear in form, we\n",
    "can make use of the `mlexp` option (see `help mlexp`). The documentation\n",
    "of `mlexp` says the following:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú573‚Äù height=‚Äú226‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "So here, since we know the LL which fulfills the conditions defined\n",
    "above, we making use of the special `mlexp` option. The `ml` is more\n",
    "generic, but we will get to it later. We can express our function as\n",
    "follows:\n",
    "\n",
    "``` if\n",
    "mlexp ((x * log({p})) + ((1 - x) * log(1 - {p})))\n",
    "```\n",
    "\n",
    "and that is all we need. A single line equation. There are of course a\n",
    "host of various other options that go with it including defining\n",
    "initialization of parameters, other constraints etc. Note in the command\n",
    "above that `x` is the Stata variable that we have generated, and `{p}`\n",
    "is the value we need to maximize. This value should always be written in\n",
    "curly brackets. There can also be more than one parameter, each in its\n",
    "own curly brackets. From the expression above, we get this output:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú687‚Äù height=‚Äú347‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "The iteration values are similar to what the optimize gives us. In fact,\n",
    "here I would like to point out that in the background, the `ml`\n",
    "functions actually use the Mata `optimize` routines. So now you know\n",
    "where all the magic happens.\n",
    "\n",
    "The value of $p$ is correctly estimated as `0.6`. The standard error is\n",
    "derived from the asymptotic variance of the finite sample, which in our\n",
    "case is very small. If we increase the number of coin tosses, then $p$\n",
    "{.mq .mr .ms .mt .mu .b} should approach 0.5, and the standard error\n",
    "should approach 0, the true values of the population distribution. We\n",
    "can check this as follows:\n",
    "\n",
    "``` if\n",
    "clear all\n",
    "set obs 1000000  // or higher or lowergen x = uniform() < .5  // a 0/1 variablemlexp ((x * log({p})) + ((1 - x) * log(1 - {p})))\n",
    "```\n",
    "\n",
    "which gives us:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú436‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "The command `mlexp` also has several post estimation options for\n",
    "conducting various tests, linear combination of parameters, creating\n",
    "margins plot etc. See `help mlexp postestimation` for more details. See\n",
    "also\n",
    "<a href=\"https://blog.stata.com/2016/06/14/multiple-equation-models-estimation-and-marginal-effects-using-mlexp/\" class=\"cc le\">this Stata blog entry</a>\n",
    "for a more advanced application of `mlexp`.\n",
    "\n",
    "Now on to more standard applications.\n",
    "\n",
    "# Logit regressions {#2366 .mv .mw .ds .aw .ax .mx .my .hl .mz .na .nb .hp .nc .nd .ne .nf .ng .nh .ni .nj .nk .nl .nm .nn .no .np .ep selectable-paragraph=‚Äú‚Äú}\n",
    "\n",
    "Another common example of MLEs are Logit or Probit regressions. These\n",
    "are standard tools when dealing with dichotomous outcomes that can be\n",
    "explained by some independent set of variables. For binary outcomes, a\n",
    "linear regression is obviously a bad choice here since it will predict\n",
    "outcomes that are fall outside the plausible \\[0,1\\] range. This is\n",
    "where the S-shaped Logit and Probit functions are really handy since\n",
    "they contain the outcomes within this range. From these models, we\n",
    "basically recover the *probability* of having a value of one. Again this\n",
    "theory you should either know or read up on.\n",
    "\n",
    "The likelihood of the Logit is derived from a logistic distribution,\n",
    "which takes the following functional form:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú478‚Äù height=‚Äú127‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "While this formula might seem complex, it follows the same logic as the\n",
    "coin toss example. Here we have have a probability if `y = 1`:\n",
    "\n",
    "``` if\n",
    "Pr(y = 1) = exp(z) / (1 + exp(z))\n",
    "```\n",
    "\n",
    "and a probability for `y = 0`:\n",
    "\n",
    "``` if\n",
    "Pr(y = 0) = 1 - exp(z) / (1 + exp(z))\n",
    "          = 1 / (1 + exp(z))\n",
    "```\n",
    "\n",
    "The likelihood formula shown above is the joint probability distribution\n",
    "of the binary outcome variable *y*.\n",
    "\n",
    "The likelihood of the Probit model is derived from a normal\n",
    "distribution, that looks like this expression:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú124‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "In both the likelihood distributions, `z = X beta` is a matrix of\n",
    "explanatory variables and `n` are the number of observations going from\n",
    "1 to `N`.\n",
    "\n",
    "Since these formulas are a pain to solve ‚Äúby hand‚Äù especially the Probit\n",
    "model, I will stick to the Logit since it is the easier one to deal\n",
    "with. Like the coin toss example, I will use a custom set of values to\n",
    "keep this relatively tractable. Let‚Äôs generate five observations:\n",
    "\n",
    "``` if\n",
    "clear allset obs 5gen y = .\n",
    "gen x = .replace y = 1 in 1\n",
    "replace y = 0 in 2\n",
    "replace y = 0 in 3\n",
    "replace y = 1 in 4\n",
    "replace y = 1 in 5replace x = -1 in 1\n",
    "replace x =  1 in 2\n",
    "replace x = -4 in 3\n",
    "replace x =  8 in 4\n",
    "replace x = -5 in 5\n",
    "```\n",
    "\n",
    "where `y` is the outcome variable, and `x` is the explanatory variable.\n",
    "There can be many more explanatory variables but since we want to graph\n",
    "the likelihood functions, we just use a single explanatory variable. In\n",
    "our model, we also include an intercept, such that `y` is explained as a\n",
    "logistic function of the `z = alpha + beta x`.\n",
    "\n",
    "For the observations above, the likelihood function can be derived as:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú682‚Äù height=‚Äú100‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "Since this is a pain to differentiate and solve for `z={alpha, beta}`,\n",
    "we therefore make use of the log likelihood (LL) expression:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú71‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "which seems a bit more benign than the likelihood. The generic logit LL\n",
    "can be written as:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú512‚Äù height=‚Äú86‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "Since we have the LL explain by two parameters `alpha` and `beta`, we\n",
    "can also visualize this as follows:\n",
    "\n",
    "<figure>\n",
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACYAAAAQCAIAAAD1fJpJAAADPElEQVR42qxUz4scVRCuV6/f6+7pxpE1O5vNIsRcEiV4ExQ8KSgqCF6Cd/HgxYP4fwiCd1FRQSTk4A+8CHvQ9ZKLOWTZGJbsrNnNus32znT3zPtRJTMvNJM22T2Y79RUv1dfVX31voiZ4fHBWquUOvkM/h8CIqqqqiXz3iulTu1BhBPOOWaWUgIAM1trHzgkRPgoiiKdIwSZ2TlnjAkRANBaE9FwOFxZWUmSJFy01iJiSA4AUZs3iqKW3hgTx7HWWgjhzLTc3vzrp6t2a0uORuaw7DGn51b92bMXPvgoXl6N49h73/Yq5hiNRm2hdV3ned5SziplZu99FN2nd86Uxe17278Xt68fD2/x/pDLUmOSoRQECQIBGJSWQVOdvfTG+Tc/Tp98GgCklC3NIiaTCQAkSdIdLCL+U964u/8bNxsJj6XT0hA5V9zbm1bHaI0klgiMQkZgQSjFhACSLdOlV7/pLz0vUT9UvLquhRDt8O9TNk0TQs6braOre/SLJpEiKBTSU6Yxj4GcYZ7cvHNrQpMIQTCQI0fAxAhsHSVPvPbKc5/9t882+QOURVEsLS210T+Pf7xhr0nySkBPCwWcxSIFQM95jNt/D3f2C9QExNVU+2mvsY4lVDzO+xffu/xpXy91uuz1el3KThQArh+u/6q+QiLFnGeIhvJcCsPILIWIKN3bitZ/HhD3ZCzznooiOKoOCY6OVm9++fbXucpO6dI5165PADMT8x/Nxhf4LXiLiiXDyvTMFf3ueXGJBSEKicIYh7Olmb0BxNkr39zZ+W59/c7h3fdff+uFi88CQFVVaZqGTe6uDzMLIYwxM49APDg4WD6z/Ln9YSPaPAf9D/HKQD/10AXxczjntNY4x7iqP/n+2jsvv3j5wjO7u7tra2vdLsNTaZpGa73oWCE+9TZV8cmeEsptr8xuTadSynJUAbnBYNClbOGcC74VphRcjYg6Yz+VfqYLkbWWiBBxUctuomiO8XicJMmjnvZi6nawiOjmqKoqy7IgbZqmRBSUemSXHYWcc0F2RPTeE1Fd11mWMbNSKniWEMJaq7VerOOEWk+iDPJYa8uy7Pf7i9bc2XDvffvrVPwbAAD//1V9Dd+xbmqNAAAAAElFTkSuQmCC\" class=\"t u v it aj oc od jp zq\" role=\"presentation\" width=\"700\" height=\"311\" alt=\"Figure generated in Mathematica 12.1\" /><figcaption aria-hidden=\"true\">Figure generated in Mathematica 12.1</figcaption>\n",
    "</figure>\n",
    "\n",
    "where we can see that the LL has a maximum point somewhere in the red\n",
    "zone.\n",
    "\n",
    "In order to solve this in Stata, we make use of the standard `ml`\n",
    "functions (see `help ml`). For the logit case, we need define a program\n",
    "as follows:\n",
    "\n",
    "``` if\n",
    "cap prog drop mllogitprogram define mllogit\n",
    "     args lnf Xb\n",
    "     quietly replace `lnf' = ln(    invlogit(`Xb')) if $ML_y1==1\n",
    "     quietly replace `lnf' = ln(1 - invlogit(`Xb')) if $ML_y1==0\n",
    "end\n",
    "```\n",
    "\n",
    "Based on the code above, the `ml` arguments are described as follows.\n",
    "The program requires arguments `args` where we provide two `lnf` and\n",
    "`Xb`. These could be any names but are suggested in Stata documentation\n",
    "for clarity. The first `lnf` is an abbreviation for ‚Äúlinear form‚Äù since\n",
    "we have a linear LL model here. The second one `Xb` is the generic term\n",
    "for covariates. We specify the conditions for the dependent variable\n",
    "*y=0* and *y=1* separately. Here *y* is denoted by the generic term\n",
    "`$ML_y1`. The distribution `invlogit` is a built-in function where\n",
    "`invlogit(x) = exp(x)/(1 + exp(x))`. An alternative way to specify the\n",
    "the above conditions would have been:\n",
    "\n",
    "``` if\n",
    "program define mllogit2\n",
    "     args lnf Xbquietly replace `lnf' =       - ln(1+exp(-`Xb')) if $ML_y1==1\n",
    "quietly replace `lnf' = -`Xb' - ln(1+exp(-`Xb')) if $ML_y1==0end\n",
    "```\n",
    "\n",
    "which is a bit more verbose, but this is up to taste. More advanced\n",
    "programmers would save the program in a separate ado file which is\n",
    "called in a do file. But for now, we keep this program itself inside the\n",
    "dofile to avoid complicating the whole thing further. Note for\n",
    "optimization, it does not matter if you use `log` or `ln`. It won‚Äôt\n",
    "change the results.\n",
    "\n",
    "Next step we need to call this program in the `ml` instance. This is\n",
    "done as follows:\n",
    "\n",
    "``` if\n",
    "ml model lf mllogit (y = x)\n",
    "```\n",
    "\n",
    "Here **lf** is the evaluator for the linear form. There are several\n",
    "other evaluators built inside Stata, which are used for the `optimize`\n",
    "functions as well. I won‚Äôt go in their details here since it requires\n",
    "introducing some more theory, but basically `lf` corresponds to `lf0`\n",
    "(the default) which estimates the parameters, `lf1` estimates the\n",
    "parameters plus the gradients (first derivative), and `lf2` calculates\n",
    "the parameters, gradients, and the Hessian (second derivative). These\n",
    "require additional inputs in the `ml` functions and are used for more\n",
    "complex calculations especially if the functions are highly non-linear\n",
    "or some custom initializations are required, or if there are\n",
    "restrictions on the gradients and the Hessians.\n",
    "\n",
    "After we use the above `ml` command, a bunch of options open up. You can\n",
    "try these and also look up their documentations:\n",
    "\n",
    "``` if\n",
    "ml check  // conducts tests to see if the ML is properly set up\n",
    "ml query  // a complex summary of the problem set upml search // searches for decent initial values\n",
    "ml plot x // same as search but uses a graphical interface\n",
    "ml graph  // graphs the log likelihood value and the iteration\n",
    "```\n",
    "\n",
    "In order to actually solve the model, we can write:\n",
    "\n",
    "``` if\n",
    "ml maximize  // or minimize depending on the setup\n",
    "```\n",
    "\n",
    "which gives us this output:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú672‚Äù height=‚Äú350‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "where we get the intercept (`alpha`) and the slope of x variable\n",
    "(`beta`). We can also check this against the standard logit command:\n",
    "\n",
    "``` if\n",
    "logit y x\n",
    "```\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú634‚Äù height=‚Äú336‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "which also gives the same output but with fewer iterations. Here the\n",
    "`logit` (or the `logistic`, both commands are same in Stata), also\n",
    "utilizes the `ml` option in the background to derive these results.\n",
    "\n",
    "Just as an additional point, and since several people struggle with\n",
    "this, we can also draw the logistic curve as follows:\n",
    "\n",
    "``` if\n",
    "logit y xmat li e(b)global b0 = e(b)[1,2]\n",
    "global b1 = e(b)[1,1]display \"($b0 * x) + $b1\"cap drop yhat\n",
    "predict yhattwoway  /// \n",
    " (scatter yhat x) ///\n",
    " (function exp(($b1 * x) + $b0) / (1 +exp(($b1 * x) + $b0) ), range(-60 60)) ///\n",
    " , ytitle(\"Probability\") legend(order(1 \"Fitted values\" 2 \"Fitted curve\"))\n",
    "```\n",
    "\n",
    "which gives us this figure:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú467‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "With logit functions, one can also derive margin plots, odds ratios,\n",
    "etc. but these additional post-estimation calculations we won‚Äôt touch\n",
    "here. These can be extracted from the derived estimates and it is useful\n",
    "to refer to the Stata manual or some text books.\n",
    "\n",
    "# Probit functions\n",
    "\n",
    "Instead of the logit function, we can also use the Probit function\n",
    "derived from the normal distribution. The LL for the Probit is simply\n",
    "the log of the likelihood we defined earlier. Since the actual LL for\n",
    "our data looks fairly ugly (there won‚Äôt be enough space to even fit it\n",
    "in one or two lines), I won‚Äôt post it here but this sort of stuff is\n",
    "easy to derive symbolically in Mathematica.\n",
    "\n",
    "Just out of curiosity, if we compare the LL functions of the Probit and\n",
    "the logit, we can see the differences clearly:\n",
    "\n",
    "!\\[Logit LL is flatter than the Profit LL. Figure generated in\n",
    "Mathematica .u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú477‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "The flatter surface is the Logit LL while the more concave shape is the\n",
    "Probit LL. Here you can also see that the edges of the Probit function\n",
    "starts getting a bit rough. These are most likely out-of-sample domains\n",
    "where one starts hitting some non-linear combination of the variables.\n",
    "\n",
    "In Stata, we can define the LL for the Probit as follows:\n",
    "\n",
    "``` if\n",
    "cap prog drop mlprobitprogram mlprobit\n",
    " args lnf xb\n",
    "   quietly replace `lnf' = ln(    normal(`xb')) if $ML_y1==1\n",
    "   quietly replace `lnf' = ln(1 - normal(`xb')) if $ML_y1==0\n",
    "endml model lf mlprobit (y = x)\n",
    "ml maximize\n",
    "```\n",
    "\n",
    "which gives us:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú691‚Äù height=‚Äú591‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "This can also be compared with the standard Stata command:\n",
    "\n",
    "``` if\n",
    "probit y x\n",
    "```\n",
    "\n",
    "and to draw the graphs that compare the Probit with the Logit, we can\n",
    "use the following set of commands:\n",
    "\n",
    "``` if\n",
    "probit y xglobal p0 = e(b)[1,2]\n",
    "global p1 = e(b)[1,1]twoway  /// \n",
    " (function y = exp(($b1 * x) + $b0) / (1 +exp(($b1 * x) + $b0) ), range(-60 60)) ///\n",
    " (function y = normal(($p1 * x) + $p0), range(-60 60)) ///\n",
    " , ytitle(\"Probability\") ///\n",
    " legend(order(1 \"Logit\" 2 \"Probit\"))\n",
    "```\n",
    "\n",
    "which gives us this graph:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú467‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "Here we can see differences in the cumulative density functions (CDFs)\n",
    "for the two functions.\n",
    "\n",
    "## Using actual data {#8459 .op .mw .ds .aw .ax .qe .qf .qg .mz .qh .qi .qj .nc .qk .ql .qm .ng .qn .qo .qp .nk .qq .qr .qs .no .qt .ep selectable-paragraph=‚Äú‚Äú}\n",
    "\n",
    "Let‚Äôs try what we learned on actual data. Here we will use Stata‚Äôs\n",
    "*union* dataset, that is also frequently used in their help files for\n",
    "examples with binary outcomes.\n",
    "\n",
    "``` if\n",
    "webuse union, clear\n",
    "```\n",
    "\n",
    "Here we use a simple specification, where we explain *union* membership\n",
    "(a binary outcome) as a function of *age*, *grade* (education\n",
    "completed), (living in the) *south*, and (being) *black*.\n",
    "\n",
    "We can run a simple `logit` and `probit` in Stata as follows:\n",
    "\n",
    "``` if\n",
    "logit union age grade south black\n",
    "probit union age grade south black\n",
    "```\n",
    "\n",
    "In ML form we can use the functions we defined above:\n",
    "\n",
    "``` if\n",
    "cap prog drop mllogit\n",
    "program define mllogit\n",
    "     args lnf Xb\n",
    "     quietly replace `lnf' = ln(    invlogit(`Xb')) if $ML_y1==1\n",
    "     quietly replace `lnf' = ln(1 - invlogit(`Xb')) if $ML_y1==0\n",
    "endcap prog drop mlprobit\n",
    "program mlprobit\n",
    " args lnf xb\n",
    "    quietly replace `lnf' = ln(    normal(`xb')) if $ML_y1==1\n",
    "    quietly replace `lnf' = ln(1 - normal(`xb')) if $ML_y1==0\n",
    "endml model lf mllogit (union = age grade south black)\n",
    "ml maximizeml model lf mlprobit (union = age grade south black)\n",
    "ml maximize\n",
    "```\n",
    "\n",
    "The code above will spit out the parameter values that maximize the LL\n",
    "function. You can also check the results against the standard commands.\n",
    "As mentioned earlier, the Stata `probit` and `logit` commands utilize\n",
    "the `ml` functions, which themselves uses the Mata `optimize`. The\n",
    "inter-connectedness of these routines also helps give a picture of how\n",
    "various different functions are built on top of each other. And so\n",
    "running `logit y x1 x2‚Ä¶` seems like a fairly easy command to use, there\n",
    "is a lot happening in the background which makes it easy to use!\n",
    "\n",
    "# Other ML functions? {#9ea5 .mv .mw .ds .aw .ax .mx .my .hl .mz .na .nb .hp .nc .nd .ne .nf .ng .nh .ni .nj .nk .nl .nm .nn .no .np .ep selectable-paragraph=‚Äú‚Äú}\n",
    "\n",
    "Since there are tons of distributions that one can apply to various data\n",
    "problems, there are tons of MLE functions as well. This\n",
    "<a href=\"https://stats.idre.ucla.edu/stata/code/imple-linear-and-nonlinear-models-using-statas-ml-command/\" class=\"cc le\">UCLA website</a>\n",
    "lists a bunch of them. But one I would like to highlight here is the MLE\n",
    "to deal with standard regressions. Using the auto dataset, this takes\n",
    "the following form:\n",
    "\n",
    "``` if\n",
    "sysuse auto, clearcapture program drop myols\n",
    "program myols\n",
    "  \n",
    "args lnf Xb lnsigma\n",
    "local y \"$ML_y1\"\n",
    "quietly replace `lnf' = ln(normalden(`y', `Xb', exp(`lnsigma')))\n",
    "endml model lf myols (xb: mpg = weight foreign) (lnsigma:)\n",
    "ml maximize\n",
    "display exp([lnsigma]_cons)\n",
    "```\n",
    "\n",
    "which gives this output:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú700‚Äù height=‚Äú704‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "where the `lnsigma` term is the mean squared error (MSE) of the\n",
    "regression. Now if we compare it with the standard OLS command:\n",
    "\n",
    "``` if\n",
    "regress mpg weight foreign\n",
    "```\n",
    "\n",
    "we get this output:\n",
    "\n",
    ".u .v .it .aj .oc .od .jp .zq width=‚Äú606‚Äù height=‚Äú296‚Äù\n",
    "role=‚Äúpresentation‚Äù}\n",
    "\n",
    "Note here that the coefficients are exactly the same but standard errors\n",
    "and the MSE terms are different. Here, unlike `logit` and `probit`,\n",
    "which also utilize `optimize` and hence results are similar, for OLS the\n",
    "estimations routes are very different. The `regress` command use\n",
    "standard matrix algebra while the `ml` option above is utilizing\n",
    "asymptotic properties of the estimators. Even though the differences are\n",
    "very small, the `ml` results should be considered better (or BLUEr) than\n",
    "the OLS estimates.\n",
    "\n",
    "There are still several things not covered in this guide but are worth\n",
    "looking up if you are serious about learning `ml`. These include more\n",
    "complex functional forms, constraints on MLE functions, optimization\n",
    "techniques (Newton-Rhapson, Berndt-Hall-Hall-Hausman etc.), search\n",
    "bounds etc. Currently there is only one book by Gould, Pitblado, and Poi\n",
    "which covers this topic in detail:"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
