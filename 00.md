---
layout: default
    usemathjax: true
---

# Introduction to the maximum likelihood estimation

## An example of coin tossing

-   Coin tosses have binary possible outcomes: a Head or a Tail.
-   In statistics, this implies that coin toss outcomes are independent
    and identically distributed, or i.i.d..
-   Let the probability of getting a Head be $p$ and the probability of
    getting a Tail be $1-p$.
-   So how do we find that value of $p$?

Let's toss the coin for five times, and assume that we get the sequence:
H,T,T,H,H.
The probability of seeing this result is calculated as
$$L(p) = p(1-p)(1-p)pp = p^3(1-p)^2$$ where 333 is the number of Heads
and 2 is the number of Tails. More generally, if we have a total of $N$
tosses, out of which $n$ are Heads, then the probability is written in a
generic function form: $$L(p) = p^n(1-p)^{N-n}$$

-   Here, $L(p)$ is the likelihood of \"observing\" the data.

-   Intuitively, it makes sense to use the value, $\hat{p}$, that
    maximizes $L(p)$ to estimate $p$.

-   The maximum likelihood estimator (MLE) is

    ``` {.latex results="raw"}
      The following equation
    \begin{equation}
      \hat{p}=\arg\max_{p}L(p).
    \end{equation}
    ```

    The following equation

    ```{=latex}
    \begin{equation}
           \hat{p}=\arg\max_{p}L(p).
    \end{equation}
    ```

The estimation problem now becomes an optimization problem.

-   Here we can differentiate $L$ with respect to $p$ and set it equal
    to zero to find the optimal value of $p$ (`L'(p)`{.verbatim}
    dL/dp=0=). But since we have powers in the equation above, this will
    just give us this very ugly expression:

``` {.latex results="raw"}
\begin{equation}
  \frac{1}{n}
\end{equation}
```

If $a^2=b$ and $ b=2 $, then the solution must be either
$$ a=+\sqrt{2} $$ or $$ a=-\sqrt{2} $$.

```{=org}
#+header: :eval "no"
```
